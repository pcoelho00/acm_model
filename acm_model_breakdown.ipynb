{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing the term structure with linear regressions\n",
    "\n",
    "Tobias Adriann, Richard K. Crump, Emanuel Moench\n",
    "\n",
    "#### Notebook Author: Pedro Coelho\n",
    "\n",
    "\n",
    "### Introdução\n",
    "\n",
    "Até o começo dos anos 1990, acreditava-se que a estrutura a termo da taxa de juros era apenas a realização das expectativas do mercado para o caminho da taxa de juros futuros. Porém dos anos 1990 em diante começou a ficar claro que os investidores exigiam um prêmio de risco para investir em títulos de prazo mais longos e quanto mais longo esse prazo, maior o prêmio. Desde então começaram a surgir modelos para estimar esse prêmio de risco não observado, como os modelos de Kim e Wright (2005) e Cochrane e Piazzesi (2005, 2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "path = str(Path().resolve())\n",
    "\n",
    "\n",
    "def plot_interest_rate_on_time(\n",
    "    curve_df: pd.DataFrame | pd.Series,\n",
    "    title: str,\n",
    "    y_tickformat: str = \".2%\",\n",
    "    rangemode: str = \"tozero\",\n",
    "    y_dtick: float = 0.02,\n",
    "    template: str = \"simple_white\",\n",
    "    xaxis_title: str = \"Dates\",\n",
    "    yaxis_title: str = \"Interest Rate\",\n",
    "    use_area_chart: bool = False,\n",
    "    showlegend: bool = True,\n",
    "):\n",
    "    if use_area_chart:\n",
    "        fig = px.area(curve_df, title=title, template=template)\n",
    "    else:\n",
    "        fig = px.line(curve_df, title=title, template=template)\n",
    "    return (\n",
    "        fig.update_yaxes(\n",
    "            tickformat=y_tickformat,\n",
    "            rangemode=rangemode,\n",
    "            dtick=y_dtick,\n",
    "            showgrid=True,\n",
    "        )\n",
    "        .update_xaxes(\n",
    "            tickformat=\"%b %Y\",\n",
    "            ticklabelmode=\"period\",\n",
    "            showgrid=True,\n",
    "        )\n",
    "        .update_layout(\n",
    "            xaxis_title=xaxis_title,\n",
    "            yaxis_title=yaxis_title,\n",
    "            showlegend=showlegend,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1,\n",
    "                title=\"\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_interest_rate_curve(\n",
    "    curve_df: pd.DataFrame | pd.Series,\n",
    "    title: str,\n",
    "    y_tickformat: str = \".2%\",\n",
    "    rangemode: str = \"tozero\",\n",
    "    y_dtick: float = 0.01,\n",
    "    template: str = \"simple_white\",\n",
    "    xaxis_title: str = \"Years\",\n",
    "    yaxis_title: str = \"Interest Rate\",\n",
    "    use_area_chart: bool = False,\n",
    "    showlegend: bool = True,\n",
    "):\n",
    "    if use_area_chart:\n",
    "        fig = px.area(curve_df, title=title, template=template)\n",
    "    else:\n",
    "        fig = px.line(curve_df, title=title, template=template)\n",
    "    return (\n",
    "        fig.update_yaxes(\n",
    "            tickformat=y_tickformat,\n",
    "            rangemode=rangemode,\n",
    "            dtick=y_dtick,\n",
    "            showgrid=True,\n",
    "        )\n",
    "        .update_xaxes(\n",
    "            rangemode=\"tozero\",\n",
    "            dtick=1,\n",
    "            showgrid=True,\n",
    "        )\n",
    "        .update_layout(\n",
    "            xaxis_title=xaxis_title,\n",
    "            yaxis_title=yaxis_title,\n",
    "            showlegend=showlegend,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1,\n",
    "                title=\"\",\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos afins de taxas de juros \n",
    "\n",
    "Geralmente partem de 3 hipóteses:\n",
    "\n",
    "1. A taxa estocástica de desconto é exponencialmente afim em relação aos choques econômicos;\n",
    "2. O preço do risco é afim em relação as variáveis do modelo;\n",
    "3. E os erros/inovações do modelo e o log das taxas de juros são condicionalmente gaussianas.\n",
    "\n",
    "Empiricamente eles são estimados através de métodos de máxima verossimilhança que são capazes de explorar as hipóteses de distribuição e as restrições de não arbitragem.\n",
    "\n",
    "O Modelo ACM também é um modelo afim para estimar a estrutura a termo da taxa de juros, com a diferença que é estimado através de Minimos Quadrados Ordinários. A grande diferença entre os modelos de máxima verossimilhança e o modelo de ACM é a hipótese sobre autocorrelação dos erros. O primeiro assume que não há autocorrelação dos erros o que leva a perda de previsibilidade dos retornos que não é capturado pelos fatores de apreçamento, já o modelo de ACM, que é baseado em MQO, não há necessidade de se fazer nenhuma hipótese sobre autocorrelação dos erros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hipóteses do modelo de ACM\n",
    "\n",
    "1. Primeiramente eles assumem que as variáveis ${X}_{t}$ de tamanho K x 1 evoluem de acordo com um modelo de Vetores Autorregressivos:\n",
    "\n",
    "    ${X}_{t+1} = {\\mu} + {\\Phi}{X}_{t} + {\\nu}_{t+1}$\n",
    "\n",
    "2. Dada as condições de não-arbitragem de um modelo de taxa de juros deve existir uma taxa estocástica de desconto ${M}_{t}$ tal que:\n",
    "\n",
    "    ${P}_{t}^{(n)} = {E}_{t}[{M}_{t+1}{P}_{t+1}^{(n-1)}]$\n",
    "\n",
    "    sendo que a taxa estocástica de desconto é exponencialmente afim em relação aos choques da economia\n",
    "\n",
    "    ${M}_{t+1} = exp({-}{r}_{t} -\\frac{1}{2}{\\lambda}_{t}'{\\lambda}_{t} - {\\lambda}_{t}'{\\Sigma}^{-\\frac{1}{2}}{\\nu}_{t+1})$\n",
    "\n",
    "    e ${r}_{t} = ln {P}_{t}$ é a taxa livre de risco continuamente composta.\n",
    "    \n",
    "3. Eles também assumem que o preço do risco é afim em relação as variáveis do modelo como o sugerido por Duffee (2002):\n",
    "\n",
    "    ${\\lambda}_{t} = {\\Sigma}^{-\\frac{1}{2}}({\\lambda}_{0} + {\\lambda}_{1}{X}_{t})$\n",
    "\n",
    "4. E assumem que os erros observados ${\\nu}_{t+1}$ são condicionalmente gaussianas\n",
    "\n",
    "    ${\\nu}_{t+1}|{({X}_{s})}_{s=0}^{t} \\sim {N(0, \\Sigma)}$\n",
    "\n",
    "\n",
    "Sendo ${rx}_{t+1}^{(n-1)}$ o log do excesso de retorno de um título com vencimento em n períodos tal que:\n",
    "\n",
    "${rx}_{t+1}^{(n-1)} = ln({P}_{t+1}^{(n-1)}) - ln({P}_{t}^{(n)}) - {r}_{t}$\n",
    "\n",
    "Com um pouco de álgebra e utilizando as hipóteses iniciais chegamos a seguinte equação:\n",
    "\n",
    "${rx}_{t+1}^{(n-1)} = {\\beta}^{(n-1)'}({\\lambda}_{0} + {\\lambda}_{1}{X}_{t}) -\\frac{1}{2}({\\beta}^{(n-1)'}{\\Sigma}{\\beta}^{(n-1)} + {\\sigma}^{2}) +{\\beta}^{(n-1)'}{\\nu}_{t+1} + {e}_{t+1}^{(n-1)}$\n",
    "\n",
    "Assumindo que ${e}_{t+1}^{(n-1)}$ é independente e identicamente distribuído com variância ${\\sigma}^{2}$\n",
    "\n",
    "Quebrando a equação acima, o retorno em excesso de um título com vencimento em n em t+1 é igual a:\n",
    "\n",
    "* ${\\beta}^{(n-1)'}({\\lambda}_{0} + {\\lambda}_{1}{X}_{t})$: o retorno esperado do título com vencimento em n no período t\n",
    "\n",
    "* $-\\frac{1}{2}({\\beta}^{(n-1)'}{\\Sigma}{\\beta}^{(n-1)} + {\\sigma}^{2})$: menos um ajuste de convexidade, necessário em modelos lineares de taxa de juros\n",
    "\n",
    "* ${\\beta}^{(n-1)'}{\\nu}_{t+1}$: mais o retorno adicional ímplicito no preço do título \n",
    "\n",
    "* ${e}_{t+1}^{(n-1)}$: e o erro de apreçamento.\n",
    "\n",
    "Ao empilhar a equação acima para todos os vencimentos e para todos os períodos de tempo obtemos:\n",
    "\n",
    "$rx = {\\beta}({\\lambda}_{0}{i}_{T}' + {\\lambda}_{1}{X\\_}) -\\frac{1}{2}({B}^{*}vec{\\Sigma} + {\\sigma}^{2}{i}_{N}){i}_{T}' + {\\beta'}V + E$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A estimação do modelo ACM é dividida em 3 etapas:\n",
    "\n",
    "1. Primeiro os fatores do modelo são decompostos em componentes previsíveis e fatores de inovação através \n",
    "da regressão dos fatores em relação aos mesmos fatores defasados. \n",
    "\n",
    "2. Na segunda etapa é estimada a exposição dos retornos em excesso em relação aos fatores defasados e em relação \n",
    "aos fatores de inovação contemporâneos.\n",
    "\n",
    "3. Finalmente é obtido os preços de mercado do risco atráves de uma regressão *cross-sectional* das exposições dos retornos em relação aos fatores de apreçamento defasados e os fatores de inovações contemporâneos.\n",
    "\n",
    "A grande vantagem do modelo de ACM é que ele é estimado através de um MQO que é computacionalmente muito mais rápido que os modelos de verossimilhança que não possuem fórmulas fechadas e exigem soluções numéricas mais complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Começamos extraindo a curva de juros de DI1 desde janeiro de 2011 até abril de 2021\n",
    "\n",
    "Neste caso a curva foi interpolada previamente para todos os dias úteis do período e para vértices diários de até 10 anos. \n",
    "\n",
    "A curva DI1, apesar de ser uma curva de futuros, é uma excelente proxy da taxa zero brasileira por ser mais líquida e possuir mais vértices que a curva de títulos públicos brasileiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_count = 12\n",
    "\n",
    "first_date = pd.to_datetime(\"2011-01-01\")\n",
    "last_date = pd.to_datetime(\"2021-04-30\")\n",
    "\n",
    "# uploading the curve date\n",
    "curve = pd.read_csv(f\"{path}/data/di1_monotonic_curve.csv\", index_col=0)\n",
    "# making sure the data has the correct data type\n",
    "curve = curve.astype(float)\n",
    "# converting the columns names to monthly\n",
    "curve.columns = [int(column) for column in curve.columns]\n",
    "# converting the date index to a datetime index\n",
    "curve.index = pd.to_datetime(curve.index)\n",
    "# filtering the data until the last date\n",
    "curve = curve[curve.index <= last_date]\n",
    "curve = curve[curve.index > first_date]\n",
    "\n",
    "print(f\"Min Date: {curve.index.min():%Y-%b-%d}, Max Date: {curve.index.max():%Y-%b-%d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraída a curva original nos filtramos a curva para uma periodicidade mensal, usando o último dia útil de cada mês e também filtramos os vértices da curva para que também sejam somente mensais. É importante que os vértices tenham a mesma periodicidade para facilitar no cálculo do excesso de retorno da curva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_curve = curve.resample(\"BME\").last()\n",
    "fig_original_curve = plot_interest_rate_on_time(\n",
    "    monthly_curve.loc[:, 60],\n",
    "    title=f\"{str(int(60 / 12))} Year Yield\",\n",
    "    y_dtick=None,\n",
    "    showlegend=False,\n",
    ")\n",
    "fig_original_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a curva mensal podemos calcular o excesso de retorno:\n",
    "\n",
    "${rx}_{t+1}^{(n-1)} = ln({P}_{t+1}^{(n-1)}) - ln({P}_{t}^{(n)}) - {r}_{t}$\n",
    "\n",
    "onde ${r}_{t}$ é a taxa livre de risco no instante *t*\n",
    "\n",
    "Aqui nos conseguimos observar a importância dos vértices e das datas terem a mesma periodicidade. Também é importante notar que nossa taxa livre de risco também segue a periodicidade da curva, se a curva fosse diária a taxa livre de risco seria o CDI, como nesse caso ela á mensal consideramos o primeiro mês de vencimento como a taxa livre de risco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_excess_returns(df: pd.DataFrame, base_count: int) -> pd.DataFrame:\n",
    "    \"\"\"Calculate the yield curve excess returns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The yield curve time series.\n",
    "        base_count (int): The base count of the yield curve\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    excess_returns = pd.DataFrame(\n",
    "        index=df.index,\n",
    "        columns=df.columns,\n",
    "        dtype=\"float64\",\n",
    "    )\n",
    "    # short-term risk-free rate is the shortest rate\n",
    "    r_t = df.iloc[:, 0].shift(1) * (1 / base_count)\n",
    "    # skipping the first column which is the short-term rate\n",
    "    for n in df.columns[1:]:\n",
    "        # bond price with maturity n-1 at t\n",
    "        p_n_minus_1_d = -df.loc[:, n - 1] * ((n - 1) / base_count)\n",
    "        # bond price with maturity n at t - 1\n",
    "        p_n_d_minus_1 = (-df.loc[:, n] * (n / base_count)).shift(1)\n",
    "        # Excess return\n",
    "        excess_returns.loc[:, n] = p_n_minus_1_d - p_n_d_minus_1 - r_t\n",
    "\n",
    "    return excess_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertemos a curva de juros para taxas contínuas e calculamos o excesso de retorno.\n",
    "\n",
    "Importante ressaltar que no calculo do excesso de retorno nos perdemos a primeira data de observação e portanto a curva é reindexada excluindo a primeira data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the curve data to continuous compounding\n",
    "curve_exp = np.log(1 + monthly_curve)\n",
    "# calculating the excess return\n",
    "excess_returns = calculate_excess_returns(curve_exp, base_count)\n",
    "# dropping the first row of data\n",
    "excess_returns = excess_returns.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "# reindexing the base curve\n",
    "curve_exp = curve_exp.loc[excess_returns.index, :]\n",
    "\n",
    "# Setup\n",
    "# just storing the number of tenor on the curve\n",
    "n_tenors = excess_returns.shape[1]\n",
    "# storing the tenor in a list for latter use\n",
    "tenors = excess_returns.columns\n",
    "# getting the sample size for later use\n",
    "sample_size = curve_exp.shape[0] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculado os excessos de retornos agora usamos a curva exponencial para calcular os PCs.\n",
    "\n",
    "PCA é uma técnica utilizada em finanças para reduzir os fatores de modelos multifatoriais de modo a torná-los mais tratáveis. Inicialmente Scheinkman and Litterman (1991) argumentavam que somente 3 fatores são suficientes para explicar todo o comportamento da curva de juros, porém em estudos mais recentes, como os de Cochrane and Piazzesi (2005, 2008) e Duffe (2011) já mostram que são necessários mais fatores para explicar os retornos dos títulos de dívida pública americanos.\n",
    "\n",
    "Utilizamos 5 fatores como o especificado no artigo do ACM.\n",
    "\n",
    "No caso da estrutura a termo de taxa de juros a literatura já foi capaz de identificar quem são os 4 primeiros PCs, sendo eles nível, inclinação, curvatura e torção. ACM nos mostra que o 5º PC também é relevante apesar de não haver uma interpretação clara sobre o que ele representa na curva de juros.\n",
    "\n",
    "Abaixo nos calculamos os PCs e mostramos no tempo o comportamento dos fatores. Os fatores estão padronizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 - get the PCA factor series\n",
    "# the ACM paper show us that 5 factors are needed,\n",
    "n_factors = 5\n",
    "# instantiating the PCA class\n",
    "pca = PCA(n_components=n_factors)\n",
    "\n",
    "# Naming the columns, this will be useful later\n",
    "pca_columns = [\"PC\" + str(i) for i in range(1, n_factors + 1)]\n",
    "\n",
    "# Calculating the PCs from the curve, we already get the demeaned and standardized data from the PCA\n",
    "transformed_values = pca.fit_transform(curve_exp.values)\n",
    "\n",
    "# creating a DataFrame with the PCAs for better use\n",
    "pca_factors = pd.DataFrame(\n",
    "    data=transformed_values, index=curve_exp.index, columns=pca_columns\n",
    ")\n",
    "\n",
    "# Plotting the PCs on a time axis\n",
    "fig_pca = plot_interest_rate_on_time(\n",
    "    pca_factors,\n",
    "    title=\"PCs no tempo\",\n",
    "    y_tickformat=\".2f\",\n",
    "    yaxis_title=\"PC Value\",\n",
    "    y_dtick=None,\n",
    ")\n",
    "fig_pca.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é interessante observar quanto cada PC explica da variância total dos fatores. \n",
    "\n",
    "Nesse caso observamos que nível e inclinação (PC1 e PC2) já são responsáveis por mais de 99% da variância observada. \n",
    "Apesar do PC4 e PC5 apresentarem menos de 2 bps da variância da curva não há prejuízo computacional em mantê-los no cálculo e assim continuamos seguindo a especificação proposta no artigo de ACM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the explained Variance for each PC\n",
    "bar_chart = px.bar(\n",
    "    x=pca_columns,\n",
    "    y=pca.explained_variance_ratio_,\n",
    "    template=\"simple_white\",\n",
    "    text=pca.explained_variance_ratio_,\n",
    ")\n",
    "bar_chart.update_yaxes(tickformat=\".2%\")\n",
    "bar_chart.update_layout(\n",
    "    showlegend=False, xaxis_title=\"PCs\", yaxis_title=\"Pct of Explained Variance\"\n",
    ")\n",
    "bar_chart.update_traces(textposition=\"outside\", texttemplate=\"%{text:.3%}\")\n",
    "bar_chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimando o modelo de ACM\n",
    "\n",
    "### Primeiro passo, estimar os parâmetros do nosso Vetor Autoregressivo\n",
    "\n",
    "Calculados os PCs usamos eles para estimar nosso modelo de Vetor Autoregressivo:\n",
    "\n",
    "${X}_{t+1} = {\\mu} + {\\Phi}{X}_{t} + {\\nu}_{t+1}$\n",
    "\n",
    "E lembrando que os assumimos que os choques ${\\nu}_{t+1}$ são condicionalmente Gaussianas com matriz de var-covar ${\\Sigma}$:\n",
    "\n",
    "${\\nu}_{t+1}|{({X}_{s})}_{s=0}^{t} \\sim {N(0, \\Sigma)}$\n",
    "\n",
    "\n",
    "Por MQO temos que $\\hat{\\beta} = ({X}{\\_}'{X}{\\_})^{-1}{X}{\\_}'{X}$ onde ${\\mu}$ é igual a primeira coluna (as constantes da regressão) de $\\hat{\\beta}$ e ${\\Phi}$ da segunda coluna em diante.\n",
    "\n",
    "Calculados os resíduos da regressão (as inovações) $\\hat{\\nu}_{t+1}$, empilhamos os valores na matriz $\\hat{V}$ e usamos para calcular um estimador da matriz de covariância das variáveis do modelo:\n",
    "\n",
    "$\\hat{\\Sigma} = \\hat{V}\\hat{V}' / T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - VAR for the PCA equities\n",
    "\n",
    "\n",
    "def var_one(\n",
    "    Y: npt.NDArray[np.float64] | pd.DataFrame | pd.Series,\n",
    "    X: npt.NDArray[np.float64] | pd.DataFrame | pd.Series,\n",
    "    n_factors: int,\n",
    ") -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]:\n",
    "    # The VAR(1) estimator is given by equation (3.2.10) from Lutkepohl's book.\n",
    "    mat_X = np.matrix(X)\n",
    "    mat_Y = np.matrix(Y).T\n",
    "    beta_hat = mat_Y @ (mat_X.T @ np.linalg.inv(mat_X @ mat_X.T))\n",
    "    # Residuals matrix\n",
    "    res = mat_Y - (beta_hat @ mat_X)\n",
    "    # unbiased estimate of the covariance matrix\n",
    "    unbiased_cov_estimate = (res @ res.T) / (mat_X.shape[1] - n_factors - 1)\n",
    "    return beta_hat, res, unbiased_cov_estimate\n",
    "\n",
    "\n",
    "# dropping the first observations of the PCs, X_t+1\n",
    "X_t_plus_1 = pca_factors.iloc[1:].copy()\n",
    "# dropping the last observations of the PCs, X_t\n",
    "X_t = pca_factors.iloc[:-1].copy()\n",
    "# adding constant\n",
    "X_t[\"const\"] = 1\n",
    "# making sure that the constant is the first column and the CPs are in order\n",
    "X_t = X_t[[\"const\"] + pca_columns].T\n",
    "\n",
    "B_hat, V_hat, S_hat = var_one(X_t_plus_1, X_t, n_factors)\n",
    "\n",
    "# Computes matrices Mu and Phi of the VAR(1) of the paper.\n",
    "# Equation 1 of ACM paper\n",
    "Mu_hat = B_hat[:, 0]\n",
    "Phi_hat = B_hat[:, 1 : n_factors + 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo passo, regredimos o excesso de retorno em relação aos fatores defasados e o fatores contemporâneos de inovação.\n",
    "\n",
    "$rx = a{i}_{T}' + {\\beta}'\\hat{V} + c{X\\_} + E$\n",
    "\n",
    "Juntando os regressores em uma matriz $(2K + 1)\\times T$, $\\widetilde{Z} = [{i}_{T}\\hat{V}'{X\\_}']'$\n",
    "os estimadores são obtidos através da seguinte equação:\n",
    "\n",
    "$[\\hat{a}\\hat{\\beta}'\\hat{c}] = rx\\widetilde{Z}'(\\widetilde{Z}\\widetilde{Z}')^{-1}$\n",
    "\n",
    "Coletamos os erros da regressão na matriz $\\hat{E}$ e obtemos o estimador $\\hat{\\sigma}^{2} = tr(\\hat{E}\\hat{E}') / NT$. tr é o traço de uma matriz quadrada, que é a soma de todos os valores da diagonal principal da matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Excess return equation\n",
    "\n",
    "# dropping the first line\n",
    "mat_rx = excess_returns.iloc[1:].values.T.astype(float)\n",
    "\n",
    "# Collecting the regressors into the (2K + 1)xT matrix\n",
    "Z_stack = np.concatenate(\n",
    "    (\n",
    "        np.ones((1, sample_size)),\n",
    "        V_hat,\n",
    "        pca_factors.iloc[:-1].values.T,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Equation 15 of ACM\n",
    "D_hat = mat_rx @ (Z_stack.T @ np.linalg.inv(Z_stack @ Z_stack.T))\n",
    "a_hat = D_hat[:, 0]\n",
    "b_hat = D_hat[:, 1 : n_factors + 1].T\n",
    "c_hat = D_hat[:, n_factors + 1 :]\n",
    "\n",
    "# Residuals of the regression\n",
    "E_hat = mat_rx - D_hat @ Z_stack\n",
    "# We then estimate sigma^2 = trace(E_hat x E_hat.T) / (number_of_tenors * sample_size)\n",
    "sigma2_hat = np.trace(E_hat @ E_hat.T) / (n_tenors * sample_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${B}^{*} = [vec({\\beta}^{(1)}{\\beta}^{(1)'}) ... vec({\\beta}^{(N)}{\\beta}^{(N)'})]'$ que é uma matriz com tamanho ${N} x {K}^{2}$\n",
    "\n",
    "vec é a vetorização de uma matriz, onde empilhamos as colunas de uma matriz de tamanho *m x n* em um vetor coluna de tamanho *mn x 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimate of the B* matrix, defined in equation (13) of the paper\n",
    "new_shape = (1, n_factors**2)\n",
    "B_star_hat = np.zeros((n_tenors, n_factors**2))\n",
    "for i in range(0, n_tenors):\n",
    "    B_star_hat[i, :] = np.reshape(b_hat[:, i] @ b_hat[:, i].T, new_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terceiro passo, estimamos o preço do risco ${\\lambda}_{0}$ e ${\\lambda}_{1}$ através de uma regressão transversal:\n",
    "\n",
    "$\\hat{\\lambda}_{0} = (\\hat{\\beta}\\hat{\\beta}')^{-1}\\hat{\\beta}(\\hat{a} + \\frac{1}{2}(\\hat{B}^{*}vec(\\hat{\\Sigma}) + \\hat{\\sigma}_{tN}))$\n",
    "\n",
    "$\\hat{\\lambda}_{1} = (\\hat{\\beta}\\hat{\\beta}')^{-1}\\hat{\\beta}\\hat{c}$\n",
    "\n",
    "Vale notar que tanto $\\hat{\\lambda}_{0}$ quanto $\\hat{\\lambda}_{1}$ são constantes, portanto o seu valor está diretamente relacionado ao período amostral selecionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Estimate price of risk parameters\n",
    "\n",
    "# vec(Sigma_hat)\n",
    "vec_s_hat = np.reshape(S_hat, (n_factors**2, 1))\n",
    "# B_start * vec(Sigma_hat)\n",
    "b_start_vec_sigma_hat = np.dot(B_star_hat, np.reshape(S_hat, (n_factors**2, 1)))\n",
    "# sigma2_hat * (vector of ones with n tenors size)\n",
    "sigma_2_hat_n = sigma2_hat * np.ones((n_tenors, 1))\n",
    "# calculating the matrix product inv(beta_hat x beta_hat_T) x beta_hat\n",
    "bb_inv_b = np.linalg.inv(b_hat @ b_hat.T) @ b_hat\n",
    "\n",
    "l_zero_hat = bb_inv_b @ (a_hat + 0.5 * (B_star_hat @ vec_s_hat + sigma_2_hat_n))\n",
    "l_one_hat = np.dot(bb_inv_b, c_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimando a curva de juros\n",
    "\n",
    "Com os valores de $({\\Phi}, {\\Sigma}, {\\sigma}, {\\beta}, {\\lambda}_{0}, {\\lambda}_{1})$ devidamente estimados podemos calcular os preços dos títulos de cupom zero para a nossa amostra.\n",
    "\n",
    "Lembrando que dada as hipóteses feitas inicialmente o preço de um título de cupom zero é exponencialmente afim em relação as variáveis do modelo:\n",
    "\n",
    "$ln{P}_{t}^{(n)} = {A}_{n} + {B}_{n}'{X}_{t} + {\\mu}_{t}^{(n)}$\n",
    "\n",
    "Substituindo a equação acima na fórmula de excesso de retorno temos que:\n",
    "\n",
    "${rx}_{t+1}^{(n-1)} = {A}_{n-1} + {B}_{n-1}'{X}_{t+1} + {\\mu}_{t+1}^{(n-1)} - {A}_{n} - {B}_{n}'{X}_{t} - {\\mu}_{t}^{(n)} + {A}_{1} + {B}_{1}'{X}_{t} + {\\mu}_{t}^{(1)}$\n",
    "\n",
    "dado que ${X}_{t+1} = {\\mu} + {\\Phi}{X}_{t} + {\\nu}_{t+1}$ temos que:\n",
    "\n",
    "${rx}_{t+1}^{(n-1)} = {A}_{n-1} + {B}_{n-1}'({\\mu} + {\\Phi}{X}_{t} + {\\nu}_{t+1}) + {\\mu}_{t+1}^{(n-1)} - {A}_{n} - {B}_{n}'{X}_{t} - {\\mu}_{t}^{(n)} + {A}_{1} + {B}_{1}'{X}_{t} + {\\mu}_{t}^{(1)}$\n",
    "\n",
    "também definimos anteriormente a fórmula de excesso de retorno como:\n",
    "\n",
    "${rx}_{t+1}^{(n-1)} = {\\beta}^{(n-1)'}({\\lambda}_{0} + {\\lambda}_{1}{X}_{t} + {\\nu}_{t+1}) -\\frac{1}{2}({\\beta}^{(n-1)'}{\\Sigma}{\\beta}^{(n-1)} + {\\sigma}^{2}) + {e}_{t+1}^{(n-1)}$\n",
    "\n",
    "Igualando as duas fórmulas acima, assumindo que ${A}_{1} = -{\\delta}_{0}$ e ${B}_{1} = -{\\delta}_{1}$ e identificando os termos correspondentes temos que:\n",
    "\n",
    "${A}_{n} = {A}_{n-1} + {B}_{n-1}'({\\mu - {\\lambda}_{0}}) + \\frac{1}{2}({B}_{n-1}'{\\Sigma}{B}_{n-1} + {\\sigma}^{2}) - {\\delta}_{0}$\n",
    "\n",
    "${B}_{n}' = {B}_{n-1}'({\\Phi} - {\\lambda}_{1}) - {\\delta}_{1}'$\n",
    "\n",
    "${A}_{0} = 0$, ${B}_{0}' = 0$ \n",
    "\n",
    "e\n",
    "\n",
    "${\\beta}^{(n)} = {B}_{n}'$\n",
    "\n",
    "também obtemos a seguinte equação para os erros de apreçamento:\n",
    "\n",
    "${\\mu}_{t+1}^{(n-1)} - {\\mu}_{t}^{(n)} + {\\mu}_{t}^{(1)} = {e}_{t+1}^{(n-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimando a curva nominal implícita pelo modelo\n",
    "\n",
    "${A}_{1}$ e ${B}_{1}$ são os fatores que definem o primeiro vertice da nossa curva de juros, ou seja, nossa taxa livre de risco. Para chegar nos valores de ${\\delta}_{0}$ e ${\\delta}_{1}$ fazemos um regressão dos PCs em relação a taxa livre de risco sendo ${\\delta}_{0}$ a constante da nossa regressão e ${\\delta}_{1}$ os betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Equation for the Short Rate\n",
    "\n",
    "# Equation 21 of ACM calculating A_1 and B_1 when n=1\n",
    "X_star = pca_factors.copy()\n",
    "X_star[\"const\"] = 1\n",
    "X_star = X_star[[\"const\"] + [\"PC\" + str(x) for x in range(1, n_factors + 1)]].values\n",
    "\n",
    "# adjusting the risk free rate to the intended frequency\n",
    "r_one = np.dot(1 / base_count, curve_exp.iloc[:, 0].values.T)\n",
    "\n",
    "# Estimating A_1 and B_1\n",
    "Delta_hat = (np.linalg.inv(X_star.T @ X_star) @ X_star.T) @ r_one\n",
    "# A_1\n",
    "d0_hat = Delta_hat[0]\n",
    "# B_1\n",
    "d1_hat = Delta_hat[1 : n_factors + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Affine Recursions\n",
    "def affine_recursions(\n",
    "    Mu_hat: npt.NDArray[np.float64],\n",
    "    Phi_hat: npt.NDArray[np.float64],\n",
    "    Sigma_hat: npt.NDArray[np.float64],\n",
    "    sigma2_hat: npt.NDArray[np.float64],\n",
    "    lambda_0_hat: npt.NDArray[np.float64],\n",
    "    lambda_1_hat: npt.NDArray[np.float64],\n",
    "    delta_0_hat: npt.NDArray[np.float64],\n",
    "    delta_1_hat: npt.NDArray[np.float64],\n",
    "    pca_factors: pd.DataFrame,\n",
    "    pca_columns: list[str],\n",
    "    n_factors: int,\n",
    "    tenors: int,\n",
    "    sample_size: int,\n",
    "    base_count: int,\n",
    ") -> npt.NDArray[np.float64]:\n",
    "    # our Xs are the PCAs\n",
    "    X_star = pca_factors.copy()\n",
    "    # Adding the constant on the Dataframe\n",
    "    X_star[\"const\"] = 1\n",
    "    # Making sure the constant is on the first column\n",
    "    X_star = X_star[[\"const\"] + pca_columns].values\n",
    "\n",
    "    N_rec = tenors.max()\n",
    "\n",
    "    Bn = np.matrix(np.zeros((n_factors, N_rec + 1)))\n",
    "    # as per ACM B_1 = - delta_1_hat\n",
    "    Bn[:, 1] = -delta_1_hat.reshape((n_factors, 1))\n",
    "\n",
    "    An = np.matrix(np.zeros((1, N_rec + 1)))\n",
    "    # as per ACM A_1 = - delta_0_hat\n",
    "    An[:, 1] = -delta_0_hat\n",
    "\n",
    "    t_factor = np.matrix(np.zeros((sample_size, N_rec + 1)))\n",
    "    t_factor[:, 1] = base_count\n",
    "\n",
    "    for i in range(2, N_rec + 1):\n",
    "        Bn[:, i] = ((Bn[:, i - 1].T @ (Phi_hat - lambda_1_hat)) - delta_1_hat.T).T\n",
    "        An[:, i] = (\n",
    "            An[:, i - 1]\n",
    "            + Bn[:, i - 1].T @ (Mu_hat - lambda_0_hat)\n",
    "            + 0.5 * (((Bn[:, i - 1].T @ Sigma_hat) @ Bn[:, i - 1]) + sigma2_hat)\n",
    "            - delta_0_hat\n",
    "        )\n",
    "        t_factor[:, i] = base_count / i\n",
    "\n",
    "    An = np.repeat(An, sample_size, axis=0)\n",
    "\n",
    "    Xt = X_star[:, 1 : n_factors + 1].T\n",
    "\n",
    "    # calculating yields, remember that they are continuous compounded rates\n",
    "    return -np.multiply(t_factor, (An + (Bn.T @ Xt).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating the model implied curve\n",
    "model_index = pca_factors[1:].index\n",
    "\n",
    "model_implied_curve = affine_recursions(\n",
    "    Mu_hat,\n",
    "    Phi_hat,\n",
    "    S_hat,\n",
    "    sigma2_hat,\n",
    "    l_zero_hat,\n",
    "    l_one_hat,\n",
    "    d0_hat,\n",
    "    d1_hat,\n",
    "    pca_factors[1:],\n",
    "    pca_columns,\n",
    "    n_factors,\n",
    "    tenors,\n",
    "    sample_size,\n",
    "    base_count,\n",
    ")\n",
    "\n",
    "model_implied_curve = pd.DataFrame(\n",
    "    data=model_implied_curve[:, 1:],\n",
    "    index=model_index,\n",
    "    columns=list(range(1, tenors.max() + 1)),\n",
    ")\n",
    "\n",
    "model_implied_curve = np.exp(model_implied_curve) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimada a curva implícita pelo modelo, vamos comparar com a curva original para garantir que a estimação foi feita corretamente. \n",
    "\n",
    "Primeiro comparamos gráficamente as curvas nominais no último dia da amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_curve = np.exp(curve_exp.loc[model_index, :]) - 1\n",
    "nominal_curves = pd.DataFrame(\n",
    "    columns=[\"Model Implied Curve\", \"Original Curve\"],\n",
    "    index=curve_exp.columns,\n",
    ")\n",
    "date_index = curve_exp.index.max()\n",
    "nominal_curves.loc[:, \"Model Implied Curve\"] = model_implied_curve.loc[\n",
    "    date_index, :\n",
    "].values\n",
    "nominal_curves.loc[:, \"Original Curve\"] = nominal_curve.loc[date_index, :].values\n",
    "nominal_curves.index = nominal_curves.index / base_count\n",
    "\n",
    "fig6 = plot_interest_rate_curve(\n",
    "    nominal_curves,\n",
    "    title=f\"DI1 Curve and Model Implied Curve on {date_index.date()}\",\n",
    "    y_dtick=None,\n",
    ")\n",
    "fig6.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo calculamos a diferença entres as curvas no último dia da amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_difference = (model_implied_curve - nominal_curve).dropna(how=\"all\")\n",
    "model_error_on_date = pd.DataFrame(columns=[\"Model Error\"], index=nominal_curve.columns)\n",
    "model_error_on_date.loc[:, \"Model Error\"] = model_difference.loc[date_index, :].values\n",
    "model_error_on_date.index = model_error_on_date.index / 12\n",
    "\n",
    "fig7 = plot_interest_rate_curve(\n",
    "    model_error_on_date * 10000,\n",
    "    title=f\"DI1 Model Error on {date_index.date()}\",\n",
    "    y_dtick=1,\n",
    "    y_tickformat=\"0.1f\",\n",
    "    use_area_chart=True,\n",
    "    yaxis_title=\"Model Error (bps)\",\n",
    ")\n",
    "fig7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente como um último teste, escolhemos um dos vértices da curva e comparamos as estimativas no tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_months = 24\n",
    "plot_series_nominal = pd.DataFrame(\n",
    "    columns=[\"Nominal Curve\", \"Model Implied Curve\"], index=model_index\n",
    ")\n",
    "plot_series_nominal.loc[:, \"Nominal Curve\"] = nominal_curve.loc[\n",
    "    model_index, n_months - 1\n",
    "].values\n",
    "plot_series_nominal.loc[:, \"Model Implied Curve\"] = model_implied_curve.loc[\n",
    "    model_index, n_months - 1\n",
    "].values\n",
    "\n",
    "fig9 = plot_interest_rate_on_time(\n",
    "    plot_series_nominal, title=f\"{str(int(n_months / 12))} Year Yield\", y_dtick=None\n",
    ")\n",
    "fig9.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E novamente calculamos a diferença entre o modelo e a curva de juros original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nominal_curve = plot_series_nominal.loc[:, \"Nominal Curve\"]\n",
    "this_model_implied_curve = plot_series_nominal.loc[:, \"Model Implied Curve\"]\n",
    "plot_series_error = this_model_implied_curve - this_nominal_curve\n",
    "plot_series_error.name = \"Yield Error\"\n",
    "fig9_error = plot_interest_rate_on_time(\n",
    "    plot_series_error,\n",
    "    title=f\"{str(int(n_months / 12))} Year Yield Error\",\n",
    "    y_dtick=None,\n",
    "    use_area_chart=True,\n",
    ")\n",
    "fig9_error.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimando a curva neutra ao risco\n",
    "\n",
    "A partir do momento que temos todos os estimadores para a curva de juros do nosso modelo, é trivial calcularmos a curva de juros neutra ao risco, somente precisamos definir que $\\hat{\\lambda}_{0}$ e $\\hat{\\lambda}_{1}$ sejam iguais a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rny = affine_recursions(\n",
    "    Mu_hat,\n",
    "    Phi_hat,\n",
    "    S_hat,\n",
    "    sigma2_hat,\n",
    "    0,\n",
    "    0,\n",
    "    d0_hat,\n",
    "    d1_hat,\n",
    "    pca_factors[1:],\n",
    "    pca_columns,\n",
    "    n_factors,\n",
    "    tenors,\n",
    "    sample_size,\n",
    "    base_count,\n",
    ")\n",
    "\n",
    "model_index = pca_factors[1:].index\n",
    "rny = pd.DataFrame(\n",
    "    data=rny[:, 1:], index=model_index, columns=list(range(1, tenors.max() + 1))\n",
    ")\n",
    "\n",
    "rny = np.exp(rny) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculada a curva neutra ao risco, podemos compará-la com a curva estimada para os preços de mercado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.to_datetime(\"2021-04-30\")\n",
    "date_index = monthly_curve.index[-1]\n",
    "model_term_premium = (model_implied_curve - rny).dropna(how=\"all\")\n",
    "mic_on_date_curve = pd.DataFrame(\n",
    "    columns=[\"Model Implied Curve\", \"Risk Neutral Curve\"],\n",
    "    index=model_implied_curve.columns,\n",
    ")\n",
    "max_year = None\n",
    "mic_on_date_curve.loc[:, \"Model Implied Curve\"] = model_implied_curve.loc[\n",
    "    date_index, :\n",
    "].values\n",
    "mic_on_date_curve.loc[:, \"Risk Neutral Curve\"] = rny.loc[date_index, :].values\n",
    "mic_on_date_curve.index = mic_on_date_curve.index / 12\n",
    "if max_year is not None:\n",
    "    mic_plot_curve = mic_on_date_curve[mic_on_date_curve.index <= max_year]\n",
    "else:\n",
    "    mic_plot_curve = mic_on_date_curve\n",
    "\n",
    "fig8 = plot_interest_rate_curve(\n",
    "    mic_plot_curve,\n",
    "    title=f\"DI1 Curve Term Risk Premium on {date_index.date()}\",\n",
    "    y_dtick=None,\n",
    ")\n",
    "fig8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora comparamos a curva implícita pelo modelo com a curva neutra ao risco ao longo do tempo para um único vertice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_months2 = 12\n",
    "plot_series2 = pd.DataFrame(\n",
    "    columns=[\"Nominal Curve\", \"Risk Neutral Curve\"], index=model_index\n",
    ")\n",
    "plot_series2.loc[:, \"Nominal Curve\"] = nominal_curve.iloc[:, n_months2 - 1].values\n",
    "plot_series2.loc[:, \"Risk Neutral Curve\"] = rny.iloc[:, n_months2 - 1].values\n",
    "\n",
    "fig4 = plot_interest_rate_on_time(\n",
    "    plot_series2, title=f\"{str(int(n_months2 / 12))} Year Yield\", y_dtick=None\n",
    ")\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_months3 = 120\n",
    "plot_series3 = pd.DataFrame(\n",
    "    columns=[\"Nominal Curve\", \"Risk Neutral Curve\"], index=model_index\n",
    ")\n",
    "plot_series3.loc[:, \"Nominal Curve\"] = nominal_curve.iloc[:, n_months3 - 1].values\n",
    "plot_series3.loc[:, \"Risk Neutral Curve\"] = rny.iloc[:, n_months3 - 1].values\n",
    "\n",
    "fig4 = plot_interest_rate_on_time(\n",
    "    plot_series3,\n",
    "    title=f\"{str(int(n_months3 / 12))} Year Yield\",\n",
    "    y_dtick=None,\n",
    "    y_tickformat=\".2f\",\n",
    ")\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último calculamos somente o prêmio de risco do modelo para os vértices escolhidos no tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_premium_n_months1 = 24\n",
    "term_premium_name1 = f\"{str(int(term_premium_n_months1 / 12))} Year\"\n",
    "term_premium_n_months2 = 60\n",
    "term_premium_name2 = f\"{str(int(term_premium_n_months2 / 12))} Years\"\n",
    "\n",
    "term_premium = ((1 + nominal_curve) / (1 + rny) - 1).dropna(how=\"all\")\n",
    "\n",
    "term_plot_series = pd.DataFrame(\n",
    "    columns=[term_premium_name1, term_premium_name2], index=term_premium.index\n",
    ")\n",
    "term_plot_series.loc[:, term_premium_name1] = term_premium.iloc[\n",
    "    :, term_premium_n_months1 - 1\n",
    "].values\n",
    "term_plot_series.loc[:, term_premium_name2] = term_premium.iloc[\n",
    "    :, term_premium_n_months2 - 1\n",
    "].values\n",
    "\n",
    "fig5 = plot_interest_rate_on_time(term_plot_series, title=\"Yield Premium\", y_dtick=None)\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_months3 = 120\n",
    "\n",
    "plot_series4 = pd.DataFrame(\n",
    "    columns=[\"Nominal Curve\", \"Term Premium\"], index=model_index\n",
    ")\n",
    "plot_series4.loc[:, \"Nominal Curve\"] = nominal_curve.iloc[:, n_months3 - 1].values\n",
    "plot_series4.loc[:, \"Term Premium\"] = term_premium.iloc[:, n_months3 - 1].values\n",
    "\n",
    "fig6 = plot_interest_rate_on_time(\n",
    "    plot_series4, title=f\"{str(int(n_months3 / 12))} Year Yield\", y_dtick=None\n",
    ")\n",
    "fig6.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acm_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
